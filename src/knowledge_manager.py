import os
import glob
import uuid
import time
import shutil
import hashlib
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Iterable

import chromadb

import dashscope
from dashscope import TextEmbedding
import yaml

try:
    from docx import Document
except ImportError:
    Document = None

try:
    from pypdf import PdfReader
except ImportError:
    PdfReader = None

# ================= è·¯å¾„é…ç½® =================

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(CURRENT_DIR)

CONFIG_PATH = os.path.join(ROOT_DIR, "config.yaml")
SOURCE_DIR = os.path.join(ROOT_DIR, "data", "knowledge_source")  # é‡å»ºæ¥æº&å½’æ¡£ç›®æ ‡
ADD_DIR = os.path.join(ROOT_DIR, "data", "knowledge_add")        # å¢é‡æ¥æº
DB_PATH = os.path.join(ROOT_DIR, "data", "chroma_db")
COLLECTION_NAME = "abacus_knowledge"

# åˆ‡åˆ†å‚æ•°ï¼ˆå­—ç¬¦çº§çš„äºŒæ¬¡åˆ‡åˆ†ä¸Šé™ï¼›è¯­ä¹‰åˆ‡åˆ†ä¼šä¼˜å…ˆä¿æŒæ®µè½/æ ‡é¢˜/ä»£ç å—ï¼‰
CHUNK_SIZE = 900
OVERLAP = 120

# æ”¯æŒçš„æ–‡ä»¶ç±»å‹
SUPPORTED_EXTS = [".md", ".txt", ".docx", ".pdf"]

# å…¥åº“ä¸¥æ ¼æ¨¡å¼ï¼šembedding å¤±è´¥æ—¶ç›´æ¥è·³è¿‡è¯¥æ–‡ä»¶ï¼ˆä¸å†™ 0 å‘é‡æ±¡æŸ“ç´¢å¼•ï¼‰
STRICT_EMBEDDING = os.getenv("AUTOTUTORIAL_STRICT_EMBEDDING", "1").strip() not in ("0", "false", "False")

# ================= API Key =================

def setup_api_key() -> None:
    api_key = ""
    try:
        if os.path.exists(CONFIG_PATH):
            with open(CONFIG_PATH, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f) or {}
            if config.get("llm"):
                api_key = (config["llm"] or {}).get("api_key", "")
            else:
                api_key = config.get("api_key", "")
    except Exception:
        pass

    if not api_key:
        api_key = os.getenv("DASHSCOPE_API_KEY", "")

    if api_key:
        dashscope.api_key = api_key
    else:
        print("âŒ è­¦å‘Š: æœªæ‰¾åˆ° API Keyï¼Œåç»­è°ƒç”¨å°†å¤±è´¥ã€‚")

# ================= Embedding =================

@dataclass
class EmbeddingResult:
    embeddings: List[List[float]]
    ok: bool
    error: str = ""


def embed_texts(texts: List[str], batch_size: int = 5, sleep_s: float = 0.2) -> EmbeddingResult:
    """æ˜¾å¼è°ƒç”¨ embedding APIï¼Œä¾¿äºåœ¨å¤±è´¥æ—¶é€‰æ‹©è·³è¿‡è€Œä¸æ˜¯å†™ 0 å‘é‡ã€‚"""
    all_embeddings: List[List[float]] = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i: i + batch_size]
        try:
            resp = TextEmbedding.call(
                model=TextEmbedding.Models.text_embedding_v3,
                input=batch
            )
            if resp.status_code == 200:
                embeddings = [item["embedding"] for item in resp.output["embeddings"]]
                all_embeddings.extend(embeddings)
            else:
                return EmbeddingResult([], ok=False, error=f"API Error: {resp.message}")
        except Exception as e:
            return EmbeddingResult([], ok=False, error=f"Network Exception: {e}")
        time.sleep(sleep_s)
    return EmbeddingResult(all_embeddings, ok=True)

# ================= è¯»å–ä¸åˆ‡åˆ† =================

def read_file(filepath: str) -> str:
    ext = os.path.splitext(filepath)[1].lower()
    content = ""

    try:
        if ext in [".md", ".txt"]:
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()

        elif ext == ".docx":
            if Document is None:
                print(f"âš ï¸ è·³è¿‡ DOCXï¼ˆæœªå®‰è£… python-docxï¼‰: {filepath}")
                return ""
            doc = Document(filepath)
            content = "\n".join([p.text for p in doc.paragraphs])

        elif ext == ".pdf":
            if PdfReader is None:
                print(f"âš ï¸ è·³è¿‡ PDFï¼ˆæœªå®‰è£… pypdfï¼‰: {filepath}")
                return ""
            reader = PdfReader(filepath)
            pages = []
            for page in reader.pages:
                pages.append(page.extract_text() or "")
            content = "\n".join(pages)

        else:
            print(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ {ext}: {filepath}")
            return ""

    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å– {filepath}: {e}")
        return ""

    return content


def _hash_text(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()


def _stable_chunk_id(doc_relpath: str, section_path: str, chunk_index: int, chunk_text: str) -> str:
    """ç¨³å®šå¯å¤ç”¨çš„ chunk idï¼ˆç”¨äºè¯æ®å›é“¾ä¸æ–­è¨€å®¡è®¡ï¼‰ã€‚"""
    base = f"{doc_relpath}||{section_path}||{chunk_index}||{_hash_text(chunk_text)}"
    return hashlib.sha1(base.encode("utf-8", errors="ignore")).hexdigest()[:24]


def _guess_doc_type(doc_relpath: str) -> str:
    p = doc_relpath.lower()
    if any(k in p for k in ["manual", "docs", "document", "official", "abacus" + os.sep + "doc"]):
        return "official_doc"
    if any(k in p for k in ["example", "examples", "demo", "sample"]):
        return "example"
    if any(k in p for k in ["issue", "faq", "troubleshoot", "error"]):
        return "faq_issue"
    if any(k in p for k in ["paper", "article", "publication"]):
        return "paper"
    return "note"


@dataclass
class Chunk:
    text: str
    section_path: str
    span_start: int
    span_end: int


def _split_by_markdown_sections(text: str) -> List[Chunk]:
    """æŒ‰ Markdown æ ‡é¢˜åˆ‡åˆ†ï¼Œå°½é‡ä¿æŒè¯­ä¹‰å®Œæ•´ï¼›åŒæ—¶ä¿ç•™ code block åŸå­æ€§ã€‚"""
    import re
    lines = text.splitlines(keepends=True)
    chunks: List[Chunk] = []

    in_code = False
    header_stack: List[str] = []  # è®°å½• #/##/### çš„è·¯å¾„

    buf: List[str] = []
    buf_start = 0
    pos = 0

    def flush(end_pos: int):
        nonlocal buf, buf_start
        if not buf:
            return
        content = "".join(buf).strip()
        if content:
            section_path = " > ".join(header_stack) if header_stack else "(no_heading)"
            chunks.append(Chunk(text=content, section_path=section_path, span_start=buf_start, span_end=end_pos))
        buf = []

    for ln in lines:
        # code fence toggle
        if ln.strip().startswith("```"):
            in_code = not in_code
            buf.append(ln)
            pos += len(ln)
            continue

        # heading line (only when not in code)
        m = None
        if not in_code:
            m = re.match(r"^(#{1,6})\s+(.*)\s*$", ln.strip())

        if m:
            flush(pos)
            buf_start = pos

            level = len(m.group(1))
            title = m.group(2).strip()
            if level <= len(header_stack):
                header_stack[:] = header_stack[: level - 1]
            header_stack.append(title)

            buf.append(ln)
            pos += len(ln)
            continue

        buf.append(ln)
        pos += len(ln)

    flush(pos)
    return chunks


def _split_long_chunk_preserve_overlap(chunk: Chunk, max_len: int = CHUNK_SIZE, overlap: int = OVERLAP) -> List[Chunk]:
    """å¯¹è¿‡é•¿å—è¿›è¡Œå­—ç¬¦çº§äºŒæ¬¡åˆ‡åˆ†ï¼Œä½†ä¿æŒ section_path ä¸ span æ˜ å°„ã€‚"""
    t = chunk.text
    if len(t) <= max_len:
        return [chunk]

    out: List[Chunk] = []
    start = 0
    while start < len(t):
        end = min(start + max_len, len(t))
        piece = t[start:end]
        out.append(Chunk(
            text=piece,
            section_path=chunk.section_path,
            span_start=chunk.span_start + start,
            span_end=chunk.span_start + end
        ))
        if end == len(t):
            break
        start += (max_len - overlap)
    return out


def split_document(filepath: str, text: str) -> List[Chunk]:
    ext = os.path.splitext(filepath)[1].lower()

    if not text or not text.strip():
        return []

    if ext == ".md":
        base = _split_by_markdown_sections(text)
        refined: List[Chunk] = []
        for c in base:
            refined.extend(_split_long_chunk_preserve_overlap(c))
        return refined

    if ext == ".txt":
        paras = []
        spans = []
        pos = 0
        buf = []
        buf_start = 0
        for ln in text.splitlines(keepends=True):
            if ln.strip() == "":
                if buf:
                    para = "".join(buf).strip()
                    if para:
                        paras.append(para)
                        spans.append((buf_start, pos))
                    buf = []
                buf_start = pos + len(ln)
            else:
                if not buf:
                    buf_start = pos
                buf.append(ln)
            pos += len(ln)
        if buf:
            para = "".join(buf).strip()
            if para:
                paras.append(para)
                spans.append((buf_start, pos))

        refined: List[Chunk] = []
        for i, para in enumerate(paras):
            c = Chunk(text=para, section_path=f"para_{i}", span_start=spans[i], span_end=spans[i])[1]
            refined.extend(_split_long_chunk_preserve_overlap(c))
        return refined

    c = Chunk(text=text.strip(), section_path="(fulltext)", span_start=0, span_end=len(text))
    return _split_long_chunk_preserve_overlap(c)

# ================= æ‰«æä¸å…¥åº“é€šç”¨å‡½æ•° =================

def scan_files(root_dir: str) -> List[str]:
    if not os.path.exists(root_dir):
        return []
    patterns = [os.path.join(root_dir, f"**/*{ext}") for ext in SUPPORTED_EXTS]
    files: List[str] = []
    for p in patterns:
        files.extend(glob.glob(p, recursive=True))
    return files


def ingest_files(
    collection,
    files: List[str],
    ingest_root_dir: str,
    ingest_type: Optional[str] = None
) -> Tuple[int, List[str]]:
    """è¿”å›ï¼š(æ–°å¢chunkæ•°, æˆåŠŸå†™å…¥è¿‡è‡³å°‘ä¸€ä¸ªchunkçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨)ã€‚"""

    total_chunks = 0
    ingested_files: List[str] = []

    for idx, filepath in enumerate(files):
        fname = os.path.basename(filepath)
        relpath = os.path.relpath(filepath, ingest_root_dir).replace("\\", "/")
        ext = os.path.splitext(filepath)[1].lower()


        print(f"[{idx+1}/{len(files)}] å¤„ç†: {relpath}...", end="", flush=True)

        content = read_file(filepath)
        if not content.strip():
            print(" [ç©º]")
            continue

        doc_type = _guess_doc_type(relpath)
        chunks = split_document(filepath, content)
        if not chunks:
            print(" [æ— å¯åˆ‡åˆ†å†…å®¹]")
            continue

        docs: List[str] = []
        ids: List[str] = []
        metadatas: List[Dict] = []

        ingest_time = int(time.time())

        for i, c in enumerate(chunks):
            text = c.text.strip()
            if not text:
                continue

            chunk_text_hash = _hash_text(text)
            chunk_id = _stable_chunk_id(relpath, c.section_path, i, text)

            docs.append(text)
            ids.append(chunk_id)
            md: Dict = {
                "source": fname,
                "doc_path": relpath,
                "doc_type": doc_type,
                "ext": ext,
                "chunk_index": i,
                "section_path": c.section_path,
                "span_start": c.span_start,
                "span_end": c.span_end,
                "text_hash": chunk_text_hash,
                "ingest_time": ingest_time,
            }
            if ingest_type:
                md["ingest_type"] = ingest_type
            metadatas.append(md)

        if not docs:
            print(" [æ— æœ‰æ•ˆchunk]")
            continue

        emb = embed_texts(docs)
        if not emb.ok:
            msg = f" âŒ embeddingå¤±è´¥: {emb.error}"
            if STRICT_EMBEDDING:
                print(msg + "ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼šè·³è¿‡è¯¥æ–‡ä»¶ï¼‰")
                continue
            else:
                print(msg + "ï¼ˆéä¸¥æ ¼æ¨¡å¼ï¼šè·³è¿‡è¯¥æ–‡ä»¶ï¼‰")
                continue

        try:
            collection.add(documents=docs, metadatas=metadatas, ids=ids, embeddings=emb.embeddings)
            print(f" âœ… {len(docs)} ç‰‡æ®µ")
            total_chunks += len(docs)
            ingested_files.append(filepath)
        except Exception as e:
            print(f" âŒ å†™å…¥å¤±è´¥: {e}")

    return total_chunks, ingested_files

# ================= ä¸¤ç§æ¨¡å¼ =================

def rebuild_db() -> None:
    print("==========================================")
    print(" ğŸš€ AutoTutorial çŸ¥è¯†åº“é‡å»º ")
    print("==========================================")
    print(f"ğŸ’¾ æ•°æ®åº“è·¯å¾„: {DB_PATH}")
    print(f"ğŸ“‚ æ•°æ®æ¥æº: {SOURCE_DIR}")

    client = chromadb.PersistentClient(path=DB_PATH)

    try:
        client.delete_collection(COLLECTION_NAME)
        print("ğŸ§¹ å·²æ¸…ç†æ—§é›†åˆ")
    except Exception:
        pass

    print("ğŸ”Œ è¿æ¥é˜¿é‡Œäº‘ Embedding API...")
    collection = client.create_collection(name=COLLECTION_NAME)

    files = scan_files(SOURCE_DIR)
    print(f"ğŸ“¦ æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
    if not files:
        return

    total_chunks, _ = ingest_files(collection, files, ingest_root_dir=SOURCE_DIR, ingest_type="rebuild")
    print(f"\nğŸ‰ é‡å»ºå®Œæˆï¼æ€»ç‰‡æ®µ: {total_chunks}")
    print(f"ğŸ’¾ æ•°æ®åº“å·²ä¿å­˜è‡³: {DB_PATH}")


def append_db() -> None:
    print("==========================================")
    print(" ğŸ§© AutoTutorial çŸ¥è¯†åº“å¢é‡è¿½åŠ  ")
    print("==========================================")
    print(f"ğŸ’¾ æ•°æ®åº“è·¯å¾„: {DB_PATH}")
    print(f"ğŸ“‚ å¢é‡ç›®å½•: {ADD_DIR}")

    client = chromadb.PersistentClient(path=DB_PATH)
    collection = client.get_or_create_collection(name=COLLECTION_NAME)

    files = scan_files(ADD_DIR)
    print(f"ğŸ“¦ æ‰¾åˆ° {len(files)} ä¸ªå¢é‡æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
    if not files:
        return

    total_chunks, ingested_files = ingest_files(collection, files, ingest_root_dir=ADD_DIR, ingest_type="append")
    print(f"\nğŸ‰ å¢é‡è¿½åŠ å®Œæˆï¼æœ¬æ¬¡æ–°å¢ç‰‡æ®µ: {total_chunks}")
    print(f"ğŸ’¾ æ•°æ®åº“è·¯å¾„: {DB_PATH}")

    os.makedirs(SOURCE_DIR, exist_ok=True)
    moved = 0

    for src_path in ingested_files:
        rel_path = os.path.relpath(src_path, ADD_DIR)
        dst_path = os.path.join(SOURCE_DIR, rel_path)
        os.makedirs(os.path.dirname(dst_path), exist_ok=True)

        try:
            shutil.move(src_path, dst_path)
            moved += 1
        except Exception as e:
            print(f"âš ï¸ å½’æ¡£å¤±è´¥ {src_path} -> {dst_path}: {e}")

    print(f"ğŸ“‚ å·²è‡ªåŠ¨å½’æ¡£ {moved} ä¸ªæ–‡ä»¶åˆ° knowledge_sourceã€‚")

# ================= ä¸»å…¥å£ï¼šè¿è¡Œæ—¶é€‰æ‹© =================

def main() -> None:
    setup_api_key()

    print("\nè¯·é€‰æ‹©æ“ä½œæ¨¡å¼ï¼š")
    print("1) é‡å»ºï¼ˆæ¸…ç©ºå¹¶é‡å»ºé›†åˆï¼Œè¯»å– knowledge_sourceï¼‰")
    print("2) æ·»åŠ ï¼ˆå¢é‡è¿½åŠ ï¼Œè¯»å– knowledge_addï¼ŒæˆåŠŸåå½’æ¡£åˆ° knowledge_sourceï¼‰")
    choice = input("è¯·è¾“å…¥ 1 æˆ– 2ï¼š").strip()

    if choice == "1":
        print("ç¡®è®¤è¦è¿›è¡Œé‡å»ºï¼Ÿè¿™å°†æ¸…ç©ºåŸæœ‰æ•°æ®åº“ï¼Œå¹¶é‡æ–°è¯»å– knowledge_source ä¸­æ‰€æœ‰æ–‡ä»¶ã€‚")
        confirm = input("Y/Nï¼š").strip().lower()
        if confirm == "y":
            rebuild_db()
        else:
            print("å·²å–æ¶ˆã€‚")

    elif choice == "2":
        print("å·²é€‰æ‹©å¢é‡è¿½åŠ ï¼Œå³å°†è¯»å– knowledge_addâ€¦â€¦")
        append_db()

    else:
        print("âŒ æ— æ•ˆè¾“å…¥ï¼Œé€€å‡ºã€‚")


if __name__ == "__main__":
    main()
